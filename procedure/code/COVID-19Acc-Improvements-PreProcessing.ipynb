{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: RP- Spatial Accessibility of COVID-19 Healthcare Resources in Illinois Pre-Processing Script\n",
    "---\n",
    "\n",
    "This is a script that automates the data gathering and pre-processing for our reproduction of Kang et al.\n",
    "\n",
    "**Reproduction of**: Rapidly measuring spatial accessibility of COVID-19 healthcare resources: a case study of Illinois, USA\n",
    "\n",
    "Original study *by* Kang, J. Y., A. Michels, F. Lyu, Shaohua Wang, N. Agbodo, V. L. Freeman, and Shaowen Wang. 2020. Rapidly measuring spatial accessibility of COVID-19 healthcare resources: a case study of Illinois, USA. International Journal of Health Geographics 19 (1):1â€“17. DOI:[10.1186/s12942-020-00229-x](https://ij-healthgeographics.biomedcentral.com/articles/10.1186/s12942-020-00229-x).\n",
    "\n",
    "Reproduction Authors: Joe Holler, Kufre Udoh, Derrick Burt, Drew An-Pham, & Spring '21 Middlebury Geog 0323.\n",
    "\n",
    "Reproduction Materials Available at: [RP-Kang Repository](https://github.com/derrickburt/RP-Kang-Improvements)\n",
    "\n",
    "Created: `29 Jun 2021`\n",
    "Revised: `29 Jun 2021`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules\n",
    "Import necessary libraries to run this model.\n",
    "See `requirements.txt` for the library versions used for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import folium\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Directories\n",
    "\n",
    "Because we have restructured the repository for replication, we need to check our working directory and make necessary adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use to set work directory properly\n",
    "if os.getcwd() == '/home/jovyan/work/RP-Kang2020/procedure/code':\n",
    "    os.chdir('../../')\n",
    "if os.getcwd() == '/home/jovyan/work/RP-Kang2020/':\n",
    "    None \n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Plot the Street Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not os.path.exists(\"data/raw/private/Chicago_Network_Buffer.graphml\"):\n",
    "    G = ox.graph_from_place('Chicago', network_type='drive', buffer_dist = 24140.2) # pulling the drive network the first time will take a while\n",
    "    ox.save_graphml(G, 'raw/private/Chicago_Network_Buffer.graphml')\n",
    "else:\n",
    "    G = ox.load_graphml('raw/private/Chicago_Network_Buffer.graphml', node_type=str)\n",
    "ox.plot_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique counts for each road network\n",
    "# Turn nodes and edges in geodataframes\n",
    "nodes, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)\n",
    "\n",
    "# Count\n",
    "print(edges['maxspeed'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate/Pre-Process Census Data  with API\n",
    "\n",
    "*Note* you will need to download a new module called 'censusdata'\n",
    "\n",
    "To do this, open a terminal in the cybergisx environment and type:\n",
    "\n",
    "```pip install censusdata```\n",
    "\n",
    "**Note: we deviate from the original paper's methodology here bringing in a larger buffer distance of census tracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load module\n",
    "import censusdata as cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# Read in all Illinois tracts using census API\n",
    "pop_api = cd.download('acs5', 2018,\n",
    "                             cd.censusgeo([('state', '17'), ('tract', '*')]),\n",
    "                             ['B01001_001E', 'B01001_016E', 'B01001_017E', 'B01001_018E', 'B01001_019E', \n",
    "                              'B01001_020E', 'B01001_021E', 'B01001_022E', 'B01001_023E', 'B01001_024E', \n",
    "                              'B01001_025E', 'B01001_040E', 'B01001_041E', 'B01001_042E', 'B01001_043E', \n",
    "                              'B01001_044E', 'B01001_045E', 'B01001_046E', 'B01001_047E', 'B01001_048E',\n",
    "                              'B01001_049E'])\n",
    "\n",
    "# Check\n",
    "# pop_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reformat and Rename columns\n",
    "# Sum + Rename 50+ population\n",
    "pop_api['OverFifty'] = pop_api.iloc[:, 1:21].sum(axis=1)\n",
    "\n",
    "# Rename Total \n",
    "pop_api['TotalPop'] = pop_api['B01001_001E']\n",
    "\n",
    "# Drop irrelevant columns\n",
    "pop_api = pop_api.drop(pop_api.columns[0:21], axis=1)\n",
    "\n",
    "# Check\n",
    "pop_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CODE TO SAFE INTO RAW DATA FOLDER\n",
    "# Create column from index tract # -- we will need thee tract ID for a join\n",
    "pop_api['TRACTCE'] = pop_api.index\n",
    "\n",
    "# Convert to string \n",
    "pop_api['TRACTCE'] = pop_api['TRACTCE'].astype(str)\n",
    "\n",
    "# Slice last 6 digits (tract id)\n",
    "pop_api['TRACTCE'] = pop_api['TRACTCE'].str.slice(-6)\n",
    "\n",
    "# Check\n",
    "pop_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note: We are using a larger subset of data here\n",
    "len(pop_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zip Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all Illinois tracts using census API\n",
    "zip_api = cd.download('acs5', 2019,\n",
    "                             cd.censusgeo([('state', '17'), ('zip code tabulation area', '*')]),\n",
    "                             ['B01003_001E'])\n",
    "\n",
    "# check\n",
    "zip_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename population column\n",
    "pop_col = {\"B01003_001E\":\"pop\"}\n",
    "zip_api = zip_api.rename(columns=pop_col)\n",
    "\n",
    "# Create column from index tract # -- we will need thee tract ID for a join\n",
    "zip_api['ZCTA5CE10'] = zip_api.index\n",
    "\n",
    "# Convert to string \n",
    "zip_api['ZCTA5CE10'] = zip_api['ZCTA5CE10'].astype(str)\n",
    "\n",
    "# Slice last 6 digits (tract id)\n",
    "zip_api['ZCTA5CE10'] = zip_api['ZCTA5CE10'].str.slice(6,11)\n",
    "\n",
    "# Check\n",
    "zip_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(zip_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate/Pre-Process COVID-19 with Requests\n",
    "\n",
    "Download covid data that will be kjoined to zip code geographies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import geojson\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately... I have not found how to access archived COVID-119 case data, so this data is cases from 4/6/2021 - 6/30/2021 (or current date...) \n",
    "# Set file path\n",
    "fp_covid = 'https://idph.illinois.gov/DPHPublicInformation/api/COVIDExport/GetZip'\n",
    "\n",
    "# Make reqeuest\n",
    "r_covid = requests.get(fp_covid)\n",
    "\n",
    "# Save request as dataframe\n",
    "covid_cases = pd.DataFrame.from_dict(json.loads(r_covid.content))\n",
    "\n",
    "# Check\n",
    "covid_cases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change confirmed cases to cases\n",
    "cases_col = {'zip':\"ZCTA5CE10\", \"confirmed_cases\":\"cases\"}\n",
    "covid_cases = covid_cases.rename(columns=cases_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge covid case data with zip code population to normalize cases\n",
    "covid_api = covid_cases.merge(zip_api, how=\"inner\", on=\"ZCTA5CE10\")\n",
    "covid_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Census Boundary Shapefiles with FTP Site and Join to Population/Covid Case Data\n",
    "\n",
    "#### Note: Here, we extract *census tracts* and *zip code geographies* based on their spatial relationship (intersection) with the street network\n",
    "\n",
    "Census TIGER/Line shapefiles can bee accessed from ftp://ftp2.census.gov/geo/tiger/ using !wget\n",
    "\n",
    "File path for Cook County 2010 tracts: ftp://ftp2.census.gov/geo/tiger//TIGER2010/TRACT/2010/tl_2010_17031_tract10.zip\n",
    "\n",
    "File path for Illinois 2010 tracts: ftp://ftp2.census.gov/geo/tiger//TIGER2018/TRACT/tl_2018_17_tract.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check directory -- we want to downlaod the raw data directly into our pre-processing data folder\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download census tract shapefiles to data/raw/public/Pre-Processing/ for All of Chicago (017)\n",
    "if not os.path.exists('data/raw/public/Pre-Processing/tl_2018_17_tract.zip'):\n",
    "    !wget -P data/raw/public/Pre-Processing/ ftp://ftp2.census.gov/geo/tiger//TIGER2018/TRACT/tl_2018_17_tract.zip\n",
    "    # Extract shapefiles\n",
    "    !unzip -d data/raw/public/Pre-Processing/ data/raw/public/Pre-Processing/tl_2018_17_tract.zip\n",
    "    # Read in all census tracts for Illinois\n",
    "    tracts_shp = gpd.read_file('data/raw/public/Pre-Processing/tl_2018_17_tract.shp')\n",
    "else:\n",
    "    # Read in all census tracts for Illinois\n",
    "    tracts_shp = gpd.read_file('data/raw/public/Pre-Processing/tl_2018_17_tract.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set crs to WGS 84\n",
    "tracts_shp = tracts_shp.to_crs(epsg=4326)\n",
    "\n",
    "# Select only tracts from Cook countiy + its adjacent counties\n",
    "tracts_shp = tracts_shp.loc[(tracts_shp[\"COUNTYFP\"] == '031') |\n",
    "                            (tracts_shp[\"COUNTYFP\"] == '043') |\n",
    "                            (tracts_shp[\"COUNTYFP\"] == '097') |\n",
    "                            (tracts_shp[\"COUNTYFP\"] == '197') ]\n",
    "\n",
    "# Check crs\n",
    "print(tracts_shp.crs)\n",
    "\n",
    "# Check length\n",
    "print(len(tracts_shp))\n",
    "\n",
    "# Check column names\n",
    "tracts_shp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for join\n",
    "new_names = {\"GEOID10\":\"GEOID\", \"TRACTCE10\":\"TRACTCE\"}\n",
    "tracts_shp = tracts_shp.rename(columns=new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Tracts shape with Tracts Population data\n",
    "## This drops duplicate values so that we do not end up with\n",
    "atrisk_data = tracts_shp.merge(pop_api.drop_duplicates(subset=['TRACTCE']), how='inner', on=\"TRACTCE\")\n",
    "atrisk_data= atrisk_data.drop(atrisk_data.columns[5:10], axis=1)\n",
    "len(atrisk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atrisk_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check directory -- we want to downlaod the raw tract data directly into our data folder\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download zip code shapefiles to data/raw/public/Pre-Processing/ for entire US\n",
    "## I have not yet found a way to select by state before extracting\n",
    "if not os.path.exists('data/raw/public/Pre-Processing/cb_2018_us_zcta510_500k.zip'):\n",
    "    !wget -P data/raw/public/Pre-Processing/ ftp://ftp2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip\n",
    "    # Extract shapefiles\n",
    "    !unzip -d data/raw/public/Pre-Processing/ data/raw/public/Pre-Processing/cb_2018_us_zcta510_500k.zip\n",
    "    # read in zip code data\n",
    "    usa_zip = gpd.read_file('data/raw/public/Pre-Processing/cb_2018_us_zcta510_500k.shp')\n",
    "else:\n",
    "    # read in zip code data\n",
    "    usa_zip = gpd.read_file('data/raw/public/Pre-Processing/cb_2018_us_zcta510_500k.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only Illinois\n",
    "ill_zip = usa_zip.loc[(usa_zip['GEOID10'] >= '60002') & (usa_zip['GEOID10'] <= '60827')]\n",
    "\n",
    "# Set crs to WGS 84\n",
    "ill_zip = ill_zip.to_crs(epsg=4326)\n",
    "\n",
    "# Check crs\n",
    "print(ill_zip.crs)\n",
    "\n",
    "# Check length\n",
    "print(len(ill_zip))\n",
    "\n",
    "# Rename column names\n",
    "ill_zip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join covid_zip to zip code geographis \n",
    "covid_zip_geo = ill_zip.merge(covid_api, how='inner', on='ZCTA5CE10')\n",
    "# Drop extra columns\n",
    "# covid_zip_geo = covid_zip_geo.drop(covid_zip_geo.columns[5:10], axis=1)\n",
    "print(len(covid_zip_geo))\n",
    "covid_zip_geo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Chicago Shapfile and Create Hexagon Grids (500-meter resolution)\n",
    "\n",
    "commented out because have not figured how to manually create hex grids? (Should I delete?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download Illinous plac tract shapefiles to data/raw/public/Pre-Processing/ for All of Illinious (017)\n",
    "# if not os.path.exists('data/raw/public/Pre-Processing/tl_2020_17_place.zip'):\n",
    "#     !wget -P data/raw/public/Pre-Processing/ ftp://ftp2.census.gov/geo/tiger//TIGER2020/PLACE/tl_2020_17_place.zip\n",
    "#     # Extract shapefiles\n",
    "#     !unzip -d data/raw/public/Pre-Processing/ data/raw/public/Pre-Processing/tl_2020_17_place.zip\n",
    "#     # Read in all place shapefiles for Illinois\n",
    "#     place_shp = gpd.read_file('data/raw/public/Pre-Processing/tl_2020_17_place.shp')\n",
    "#     # Select only Chicago\n",
    "#     chicago_shp = place_shp.loc[place_shp['NAME']=='Chicago']\n",
    "#     # Save as shapefile\n",
    "#     chicago_shp.to_file('data/raw/public/Pre-Processing/chicago_place.shp')\n",
    "# else:\n",
    "#     # Read in all census tracts for Illinois\n",
    "#     chicago_shp = gpd.read_file('data/raw/public/Pre-Processing/chicago_place.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chicago_shp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and plot grid file for Chicago\n",
    "grid_file = gpd.read_file('./data/raw/public/GridFile/Chicago_Grid.shp')\n",
    "grid_file.plot(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_file.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital Data\n",
    "\n",
    "Note that 999 is treated as a \"NULL\"/\"NA\" so these hospitals are filtered out. This data contains the number of ICU beds and ventilators at each hospital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals = gpd.read_file('./data/raw/public/HospitalData/Chicago_Hospital_Info.shp')\n",
    "hospitals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate/Pre-Process Hospital with Requests -- Still Drafting this\n",
    "\n",
    "documentation for requests: https://docs.python-requests.org/en/master/\n",
    "\n",
    "commented out because not working (should I delete?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ADD CODE TO SAFE INTO RAW DATA FOLDER\n",
    "# # Set file paths for general care and icu beds\n",
    "# fp_gen = 'https://opendata.arcgis.com/datasets/6ac5e325468c4cb9b905f1728d6fbf0f_0.geojson'\n",
    "# fp_icu = 'https://healthdata.gov/resource/uqq2-txqb.json'\n",
    "\n",
    "# # Eequests for g eneral care and icu beds\n",
    "# r_gen = requests.get(fp_gen)\n",
    "# r_icu = requests.get(fp_icu)\n",
    "\n",
    "# # Get hospitals \n",
    "# hospitals_gen = gpd.GeoDataFrame.from_features(geojson.loads(r_gen.content),  crs=\"EPSG:26971\")\n",
    "# hospitals_icu = pd.DataFrame.from_dict(json.loads(r_icu.content))\n",
    "\n",
    "# # # Filter for icu and general care\n",
    "# hospitals_gen = hospitals_gen.loc[(hospitals_gen['STATE'] == 'IL') & (hospitals_gen['TYPE'] == 'GENERAL ACUTE CARE')]\n",
    "# # ERROR Here: it is only taking the first thousand\n",
    "# hospitals_icu = hospitals_icu[['hospital_pk', 'collection_week', 'state', 'city', 'ccn', 'hospital_name', 'zip', 'ccn', 'address', 'total_icu_beds_7_day_avg']]\n",
    "\n",
    "# # Capitalize join column \n",
    "# hospitals_icu.rename(columns={'address':'ADDRESS'}, inplace=True)\n",
    "\n",
    "# # Join\n",
    "# hospitals_api = hospitals_gen.merge(hospitals_icu, on='ADDRESS')\n",
    "\n",
    "# # Check \n",
    "# hospitals_icu['city'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Helper\" Functions\n",
    "\n",
    "The functions below are needed for our analysis later, let's take a look!\n",
    "\n",
    "### network_setting\n",
    "\n",
    "Cleans the OSMNX network to work better with drive-time analysis.\n",
    "\n",
    "First, we remove all nodes with 0 outdegree because any hospital assigned to such a node would be unreachable from everywhere. Next, we remove small (under 10 node) *strongly connected components* to reduce erroneously small ego-centric networks. Lastly, we ensure that the max speed is set and in the correct units before calculating time.\n",
    "\n",
    "Args:\n",
    "\n",
    "* network: OSMNX network for the spatial extent of interest\n",
    "\n",
    "Returns:\n",
    "\n",
    "* OSMNX network: cleaned OSMNX network for the spatial extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_setting(network):\n",
    "    _nodes_removed = len([n for (n, deg) in network.out_degree() if deg ==0])\n",
    "    network.remove_nodes_from([n for (n, deg) in network.out_degree() if deg ==0])\n",
    "    for component in list(nx.strongly_connected_components(network)):\n",
    "        if len(component)<10:\n",
    "            for node in component:\n",
    "                _nodes_removed+=1\n",
    "                network.remove_node(node)\n",
    "    for u, v, k, data in tqdm(G.edges(data=True, keys=True),position=0):\n",
    "        if 'maxspeed' in data.keys():\n",
    "            speed_type = type(data['maxspeed'])\n",
    "            if (speed_type==str):\n",
    "                # Add in try/except blocks to catch maxspeed formats that don't fit Kang et al's cases\n",
    "                try:\n",
    "                    if len(data['maxspeed'].split(','))==2:\n",
    "                        data['maxspeed_fix']=float(data['maxspeed'].split(',')[0])                  \n",
    "                    elif data['maxspeed']=='signals':\n",
    "                        data['maxspeed_fix']=30.0 # drive speed setting as 35 miles\n",
    "                    else:\n",
    "                        data['maxspeed_fix']=float(data['maxspeed'].split()[0])\n",
    "                except:\n",
    "                    data['maxspeed_fix']=30.0 #miles\n",
    "            else:\n",
    "                try:\n",
    "                    data['maxspeed_fix']=float(data['maxspeed'][0].split()[0])\n",
    "                except:\n",
    "                    data['maxspeed_fix']=30.0 #miles\n",
    "        else:\n",
    "            data['maxspeed_fix']=30.0 #miles\n",
    "        data['maxspeed_meters'] = data['maxspeed_fix']*26.8223 # convert mile to meter\n",
    "        data['time'] = float(data['length'])/ data['maxspeed_meters']\n",
    "    print(\"Removed {} nodes ({:2.4f}%) from the OSMNX network\".format(_nodes_removed, _nodes_removed/float(network.number_of_nodes())))\n",
    "    print(\"Number of nodes: {}\".format(network.number_of_nodes()))\n",
    "    print(\"Number of edges: {}\".format(network.number_of_edges()))    \n",
    "    return(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hospital_setting\n",
    "\n",
    "Finds the nearest OSMNX node for each hospital.\n",
    "\n",
    "Args:\n",
    "\n",
    "* hospital: GeoDataFrame of hospitals\n",
    "* G: OSMNX network\n",
    "\n",
    "Returns:\n",
    "\n",
    "* GeoDataFrame of hospitals with info on nearest OSMNX node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hospital_setting(hospitals, G):\n",
    "    # Create an empty column \n",
    "    hospitals['nearest_osm']=None\n",
    "    # Append the neaerest osm column with each hospitals neaerest osm node\n",
    "    for i in tqdm(hospitals.index, desc=\"Find the nearest osm from hospitals\", position=0):\n",
    "        hospitals['nearest_osm'][i] = ox.get_nearest_node(G, [hospitals['Y'][i], hospitals['X'][i]], method='euclidean') # find the nearest node from hospital location\n",
    "    print ('hospital setting is done')\n",
    "    return(hospitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pop_centroid\n",
    "\n",
    "Converts geodata to centroids\n",
    "\n",
    "Args:\n",
    "\n",
    "* pop_data: a GeodataFrame\n",
    "* pop_type: a string, either \"pop\" for general population or \"covid\" for COVID-19 case data\n",
    "\n",
    "Returns:\n",
    "\n",
    "* GeoDataFrame of centroids with population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_centroid (pop_data, pop_type):\n",
    "    pop_data = pop_data.to_crs({'init': 'epsg:4326'})\n",
    "    # If pop is selected in dropdown, select at risk pop where population is greater than 0\n",
    "    if pop_type ==\"pop\":\n",
    "        pop_data=pop_data[pop_data['OverFifty']>=0]\n",
    "    # If covid is selected in dropdown, select where covid cases are greater than 0\n",
    "    if pop_type ==\"covid\":\n",
    "        pop_data=pop_data[pop_data['cases']>=0]\n",
    "    pop_cent = pop_data.centroid # it make the polygon to the point without any other information\n",
    "    # Convert to gdf\n",
    "    pop_centroid = gpd.GeoDataFrame()\n",
    "    i = 0\n",
    "    for point in tqdm(pop_cent, desc='Pop Centroid File Setting', position=0):\n",
    "        if pop_type== \"pop\":\n",
    "            pop = pop_data.iloc[i]['OverFifty']\n",
    "            code = pop_data.iloc[i]['GEOID']\n",
    "        if pop_type ==\"covid\":\n",
    "            pop = pop_data.iloc[i]['cases']\n",
    "            code = pop_data.iloc[i].ZCTA5CE10\n",
    "        pop_centroid = pop_centroid.append({'code':code,'pop': pop,'geometry': point}, ignore_index=True)\n",
    "        i = i+1\n",
    "    return(pop_centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### djikstra_cca_polygons\n",
    "\n",
    "Function written by Joe Holler + Derrick Burt. It is a more efficient way to calculate distance-weighted catchment areas for each hospital. The algorithm runs quicker than the original one (\"calculate_catchment_area\"). It first creaets a dictionary (with a node and its corresponding drive time from the hospital) of all nodes within a 30 minute drive time (using single_cource_dijkstra_path_length function). From here, two more dictionaries are constructed by querying the original one. From this dictionaries, single part convex hulls are created for each drive time interval and appended into a single list (one list with 3 polygon geometries). Within the list, the polygons are differenced from each other to produce three catchment areas.\n",
    "\n",
    "Args:\n",
    "* G: cleaned network graph *with node point geometries attached*\n",
    "* nearest_osm: A unique nearest node ID calculated for a single hospital\n",
    "* distances: 3 distances (in drive time) to calculate catchment areas from\n",
    "* distance_unit: unit to calculate (time)\n",
    "\n",
    "Returns:\n",
    "* A list of 3 diffrenced (not-overlapping) catchment area polygons (10 min poly, 20 min poly, 30 min poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dijkstra_cca_polygons(G, nearest_osm, distances, distance_unit = \"time\"):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Before running: must assign point geometries to street nodes\n",
    "    \n",
    "    # create point geometries for the entire graph\n",
    "    for node, data in G.nodes(data=True):\n",
    "    data['geometry']=Point(data['x'], data['y'])\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ## CREATE DICTIONARIES\n",
    "    # create dictionary of nearest nodes\n",
    "    nearest_nodes_30 = nx.single_source_dijkstra_path_length(G, nearest_osm, distances[2], distance_unit) # creating the largest graph from which 10 and 20 minute drive times can be extracted from\n",
    "    \n",
    "    # extract values within 20 and 10 (respectively) minutes drive times\n",
    "    nearest_nodes_20 = dict()\n",
    "    nearest_nodes_10 = dict()\n",
    "    for key, value in nearest_nodes_30.items():\n",
    "        if value <= 20:\n",
    "            nearest_nodes_20[key] = value\n",
    "        if value <= 10:\n",
    "            nearest_nodes_10[key] = value\n",
    "    \n",
    "    ## CREATE POLYGONS FOR 3 DISTANCE CATEGORIES (10 min, 20 min, 30 min)\n",
    "    # 30 MIN\n",
    "    # If the graph already has a geometry attribute with point data,\n",
    "    # this line will create a GeoPandas GeoDataFrame from the nearest_nodes_30 dictionary\n",
    "    points_30 = gpd.GeoDataFrame(gpd.GeoSeries(nx.get_node_attributes(G.subgraph(nearest_nodes_30), 'geometry')))\n",
    "\n",
    "    # This line converts the nearest_nodes_30 dictionary into a Pandas data frame and joins it to points\n",
    "    # left_index=True and right_index=True are options for merge() to join on the index values\n",
    "    points_30 = points_30.merge(pd.Series(nearest_nodes_30).to_frame(), left_index=True, right_index=True)\n",
    "\n",
    "    # Re-name the columns and set the geodataframe geometry to the geometry column\n",
    "    points_30 = points_30.rename(columns={'0_x':'geometry','0_y':'z'}).set_geometry('geometry')\n",
    "\n",
    "    # Create a convex hull polygon from the points\n",
    "    polygon_30 = gpd.GeoDataFrame(gpd.GeoSeries(points_30.unary_union.convex_hull))\n",
    "    polygon_30 = polygon_30.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "    \n",
    "    # 20 MIN\n",
    "    # Select nodes less than or equal to 20\n",
    "    points_20 = points_30.query(\"z <= 20\")\n",
    "    \n",
    "    # Create a convex hull polygon from the points\n",
    "    polygon_20 = gpd.GeoDataFrame(gpd.GeoSeries(points_20.unary_union.convex_hull))\n",
    "    polygon_20 = polygon_20.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "    \n",
    "    # 10 MIN\n",
    "    # Select nodes less than or equal to 10\n",
    "    points_10 = points_30.query(\"z <= 10\")\n",
    "    \n",
    "    # Create a convex hull polygon from the points\n",
    "    polygon_10 = gpd.GeoDataFrame(gpd.GeoSeries(points_10.unary_union.convex_hull))\n",
    "    polygon_10 = polygon_10.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "    \n",
    "    # Create empty list and append polygons\n",
    "    polygons = []\n",
    "    \n",
    "    # Append\n",
    "    polygons.append(polygon_10)\n",
    "    polygons.append(polygon_20)\n",
    "    polygons.append(polygon_30)\n",
    "    \n",
    "    # Clip the overlapping distance ploygons (create two donuts + hole)\n",
    "    for i in reversed(range(1, len(distances))):\n",
    "        polygons[i] = gpd.overlay(polygons[i], polygons[i-1], how=\"difference\")\n",
    "\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hospital_measure_acc (adjusted to incorporate dijkstra_cca_polygons)\n",
    "\n",
    "Measures the effect of a single hospital on the surrounding area. (Uses `dijkstra_cca_polygons`)\n",
    "\n",
    "Args:\n",
    "\n",
    "* \\_thread\\_id: int used to keep track of which thread this is\n",
    "* hospital: Geopandas dataframe with information on a hospital\n",
    "* pop_data: Geopandas dataframe with population data\n",
    "* distances: Distances in time to calculate accessibility for\n",
    "* weights: how to weight the different travel distances\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Tuple containing:\n",
    "    * Int (\\_thread\\_id)\n",
    "    * GeoDataFrame of catchment areas with key stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hospital_measure_acc (_thread_id, hospital, pop_data, distances, weights):\n",
    "    # Create polygons\n",
    "    polygons = dijkstra_cca_polygons(G, hospital['nearest_osm'], distances)\n",
    "    \n",
    "    # Calculate accessibility measurements\n",
    "    num_pops = []\n",
    "    for j in pop_data.index:\n",
    "        point = pop_data['geometry'][j]\n",
    "        # Multiply polygons by weights\n",
    "        for k in range(len(polygons)):\n",
    "            if len(polygons[k]) > 0: # To exclude the weirdo (convex hull is not polygon)\n",
    "                if (point.within(polygons[k].iloc[0][\"geometry\"])):\n",
    "                    num_pops.append(pop_data['pop'][j]*weights[k])  \n",
    "    total_pop = sum(num_pops)\n",
    "    for i in range(len(distances)):\n",
    "        polygons[i]['time']=distances[i]\n",
    "        polygons[i]['total_pop']=total_pop\n",
    "        polygons[i]['hospital_icu_beds'] = float(hospital['Adult ICU'])/polygons[i]['total_pop'] # proportion of # of beds over pops in 10 mins\n",
    "        polygons[i]['hospital_vents'] = float(hospital['Total Vent'])/polygons[i]['total_pop'] # proportion of # of beds over pops in 10 mins\n",
    "        polygons[i].crs = { 'init' : 'epsg:4326'}\n",
    "        polygons[i] = polygons[i].to_crs({'init':'epsg:32616'})\n",
    "    print('\\rCatchment for hospital {:4.0f} complete'.format(_thread_id), end=\"\")\n",
    "    return(_thread_id, [ polygon.copy(deep=True) for polygon in polygons ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measure_acc_par\n",
    "\n",
    "Parallel implementation of accessibility measurement.\n",
    "\n",
    "Args:\n",
    "\n",
    "* hospitals: Geodataframe of hospitals\n",
    "* pop_data: Geodataframe containing population data\n",
    "* network: OSMNX street network\n",
    "* distances: list of distances to calculate catchments for\n",
    "* weights: list of floats to apply to different catchments\n",
    "* num\\_proc: number of processors to use.\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Geodataframe of catchments with accessibility statistics calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hospital_acc_unpacker(args):\n",
    "    return hospital_measure_acc(*args)\n",
    "\n",
    "def measure_acc_par (hospitals, pop_data, network, distances, weights, num_proc = 4):\n",
    "    catchments = []\n",
    "    for distance in distances:\n",
    "        catchments.append(gpd.GeoDataFrame())\n",
    "    pool = mp.Pool(processes = num_proc)\n",
    "    hospital_list = [ hospitals.iloc[i] for i in range(len(hospitals)) ]\n",
    "    results = pool.map(hospital_acc_unpacker, zip(range(len(hospital_list)), hospital_list, itertools.repeat(pop_data), itertools.repeat(distances), itertools.repeat(weights)))\n",
    "    pool.close()\n",
    "    results.sort()\n",
    "    results = [ r[1] for r in results ]\n",
    "    for i in range(len(results)):\n",
    "        for j in range(len(distances)):\n",
    "            catchments[j] = catchments[j].append(results[i][j], sort=False)\n",
    "    return catchments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overlap_calc\n",
    "\n",
    "Calculates and aggregates accessibility statistics for one catchment on our grid file.\n",
    "\n",
    "Args:\n",
    "\n",
    "* \\_id: thread ID\n",
    "* poly: GeoDataFrame representing a catchment area\n",
    "* grid_file: a GeoDataFrame representing our grids\n",
    "* weight: the weight to applied for a given catchment\n",
    "* service_type: the service we are calculating for: ICU beds or ventilators\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Tuple containing:\n",
    "    * thread ID\n",
    "    * Counter object (dictionary for numbers) with aggregated stats by grid ID number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def overlap_calc(_id, poly, grid_file, weight, service_type):\n",
    "    value_dict = Counter()\n",
    "    if type(poly.iloc[0][service_type])!=type(None):           \n",
    "        value = float(poly[service_type])*weight\n",
    "        intersect = gpd.overlay(grid_file, poly, how='intersection')\n",
    "        intersect['overlapped']= intersect.area\n",
    "        intersect['percent'] = intersect['overlapped']/intersect['area']\n",
    "        intersect=intersect[intersect['percent']>=0.5]\n",
    "        intersect_region = intersect['id']\n",
    "        for intersect_id in intersect_region:\n",
    "            try:\n",
    "                value_dict[intersect_id] +=value\n",
    "            except:\n",
    "                value_dict[intersect_id] = value\n",
    "    return(_id, value_dict)\n",
    "\n",
    "def overlap_calc_unpacker(args):\n",
    "    return overlap_calc(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overlapping_function\n",
    "\n",
    "Calculates how all catchment areas overlap with and affect the accessibility of each grid in our grid file.\n",
    "\n",
    "Args:\n",
    "\n",
    "* grid_file: GeoDataFrame of our grid\n",
    "* catchments: GeoDataFrame of our catchments\n",
    "* service_type: the kind of care being provided (ICU beds vs. ventilators)\n",
    "* weights: the weight to apply to each service type\n",
    "* num\\_proc: the number of processors\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Geodataframe - grid\\_file with calculated stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_function (grid_file, catchments, service_type, weights, num_proc = 4):\n",
    "    grid_file[service_type]=0\n",
    "    pool = mp.Pool(processes = num_proc)\n",
    "    acc_list = []\n",
    "    for i in range(len(catchments)):\n",
    "        acc_list.extend([ catchments[i][j:j+1] for j in range(len(catchments[i])) ])\n",
    "    acc_weights = []\n",
    "    for i in range(len(catchments)):\n",
    "        acc_weights.extend( [weights[i]]*len(catchments[i]) )\n",
    "    results = pool.map(overlap_calc_unpacker, zip(range(len(acc_list)), acc_list, itertools.repeat(grid_file), acc_weights, itertools.repeat(service_type)))\n",
    "    pool.close()\n",
    "    results.sort()\n",
    "    results = [ r[1] for r in results ]\n",
    "    service_values = results[0]\n",
    "    for result in results[1:]:\n",
    "        service_values+=result\n",
    "    for intersect_id, value in service_values.items():\n",
    "        grid_file.loc[grid_file['id']==intersect_id, service_type] += value\n",
    "    return(grid_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization\n",
    "\n",
    "Normalizes our result (Geodataframe) for a given resource (res)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization (result, res):\n",
    "    result[res]=(result[res]-min(result[res]))/(max(result[res])-min(result[res]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file_import\n",
    "\n",
    "Imports all files we need to run our code and pulls the Illinois network from OSMNX if it is not present (will take a while). \n",
    "\n",
    "**NOTE:** even if we calculate accessibility for just Chicago, we want to use the Illinois network (or at least we should not use the Chicago network) because using the Chicago network will result in hospitals near but outside of Chicago having an infinite distance (unreachable because roads do not extend past Chicago).\n",
    "\n",
    "Args:\n",
    "\n",
    "* pop_type: population type, either \"pop\" for general population or \"covid\" for COVID-19 cases\n",
    "* region: the region to use for our hospital and grid file (\"Chicago\" or \"Illinois\")\n",
    "\n",
    "Returns:\n",
    "\n",
    "* G: OSMNX network\n",
    "* hospitals: Geodataframe of hospitals\n",
    "* grid_file: Geodataframe of grids\n",
    "* pop_data: Geodataframe of population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_map(output_grid, base_map, hospitals, resource):\n",
    "    ax=output_grid.plot(column=resource, cmap='PuBuGn',figsize=(18,12), legend=True, zorder=1)\n",
    "    # Next two lines set bounds for our x- and y-axes because it looks like there's a weird \n",
    "    # Point at the bottom left of the map that's messing up our frame (Maja)\n",
    "    ax.set_xlim([314000, 370000])\n",
    "    ax.set_ylim([540000, 616000])\n",
    "    base_map.plot(ax=ax, facecolor=\"none\", edgecolor='gray', lw=0.1)\n",
    "    hospitals.plot(ax=ax, markersize=10, zorder=1, c='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "\n",
    "Below you can customize the input of the model:\n",
    "\n",
    "* Processor - the number of processors to use\n",
    "* Region - the spatial extent of the measure\n",
    "* Population - the population to calculate the measure for\n",
    "* Resource - the hospital resource of interest\n",
    "* Hospital - all hospitals or subset to check code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "\n",
    "processor_dropdown = ipywidgets.Dropdown( options=[(\"1\", 1), (\"2\", 2), (\"3\", 3), (\"4\", 4)],\n",
    "    value = 4, description = \"Processor: \")\n",
    "\n",
    "place_dropdown = ipywidgets.Dropdown( options=[(\"Chicago\", \"Chicago\"), (\"Illinois\",\"Illinois\")],\n",
    "    value = \"Chicago\", description = \"Region: \")\n",
    "\n",
    "population_dropdown = ipywidgets.Dropdown( options=[(\"Population at Risk\", \"pop\"), (\"COVID-19 Patients\", \"covid\") ],\n",
    "    value = \"pop\", description = \"Population: \")\n",
    "\n",
    "resource_dropdown = ipywidgets.Dropdown( options=[(\"ICU Beds\", \"hospital_icu_beds\"), (\"Ventilators\", \"hospital_vents\") ],\n",
    "    value = \"hospital_icu_beds\", description = \"Resource: \")\n",
    "\n",
    "hospital_dropdown =  ipywidgets.Dropdown( options=[(\"All hospitals\", \"hospitals\"), (\"Subset\", \"hospital_subset\") ],\n",
    "    value = \"hospitals\", description = \"Hospital:\")\n",
    "\n",
    "display(processor_dropdown,place_dropdown,population_dropdown,resource_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create point geometries for the entire graph (ESSENTIAL FOR CALCULATING CATCHMENT AREAS W DIJKSTRA METHOD)\n",
    "for node, data in G.nodes(data=True):\n",
    "    data['geometry']=Point(data['x'], data['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# G, hospitals, grid_file, pop_data = file_import (population_dropdown.value, place_dropdown.value)\n",
    "G = network_setting(G)\n",
    "# Modify code to react to processor dropdown (got rid of file_import function)\n",
    "if population_dropdown.value == \"pop\":\n",
    "    pop_data = pop_centroid(atrisk_data, population_dropdown.value)\n",
    "elif population_dropdown.value == \"covid\":\n",
    "    pop_data = pop_centroid(covid_data, population_dropdown.value)\n",
    "# Modify code to react to processor dropdown\n",
    "if hospital_dropdown.value == \"hospitals\":\n",
    "    hopitals = hospitals = hospital_setting(hospitals, G)\n",
    "elif hospital_dropdown.value == \"hospital_subset\":\n",
    "    hopitals = hospitals = hospital_setting(hospital_subset, G)\n",
    "hospitals = hospital_setting(hospitals, G)\n",
    "distances=[10,20,30] # Distances in travel time\n",
    "weights=[1.0, 0.68, 0.22] # Weights where weights[0] is applied to distances[0]\n",
    "# Other weighting options representing different distance decays\n",
    "# weights1, weights2, weights3 = [1.0, 0.42, 0.09], [1.0, 0.75, 0.5], [1.0, 0.5, 0.1]\n",
    "resources = [\"hospital_icu_beds\", \"hospital_vents\"] # resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check OSM Network\n",
    "\n",
    "compare to earlier speed counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Get unique counts for each road network\n",
    "# Turn nodes and edges in geodataframes\n",
    "nodes, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)\n",
    "\n",
    "# Count\n",
    "print(edges['maxspeed_fix'].value_counts())\n",
    "print(len(edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code to show that catchment areas are properly calculated\n",
    "\n",
    "allows us to see catchment areas of the subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if functions are working properly\n",
    "if hospital_dropdown.value == \"hospital_subset\":\n",
    "    # Create point geometries for entire graph\n",
    "    for node, data in G.nodes(data=True):\n",
    "        data['geometry']=Point(data['x'], data['y'])\n",
    "    \n",
    "    # Create catchment\n",
    "    poly = dijkstra_cca_polygons(G, hospitals['nearest_osm'][0], distances)\n",
    "    \n",
    "    # Reproject polygons\n",
    "    for i in range(len(poly)):\n",
    "        poly[i].crs = { 'init' : 'epsg:4326'}\n",
    "        poly[i] = poly[i].to_crs({'init':'epsg:32616'})\n",
    "        \n",
    "    # Reproject hospitals \n",
    "    hospital_subset = hospital_subset.to_crs(epsg=32616)\n",
    "    \n",
    "    ## NEED TO PROJECT THE POLYGONS SO THAT THECAN BE COMPARED WITH HOSPITAL AND STREET NETWORK\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "    poly[0].plot(ax=ax, color=\"royalblue\", label=\"10 min drive\")\n",
    "    poly[1].plot(ax=ax, color=\"cornflowerblue\", label=\"20 min drive\")\n",
    "    poly[2].plot(ax=ax, color=\"lightsteelblue\", label=\"30 min drive\")\n",
    "    \n",
    "    hospital_subset.plot(ax=ax, color=\"red\", legend=True, label = \"hospital\")\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    \n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "catchments = measure_acc_par(hospitals, pop_data, G, distances, weights, num_proc=processor_dropdown.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for j in range(len(catchments)):\n",
    "    catchments[j] = catchments[j][catchments[j][resource_dropdown.value]!=float('inf')]\n",
    "result=overlapping_function (grid_file, catchments, resource_dropdown.value, weights, num_proc=processor_dropdown.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = normalization (result, resource_dropdown.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & Discussion\n",
    "\n",
    "to be written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous Accessibility Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hospitals = hospitals.to_crs({'init': 'epsg:26971'})\n",
    "result = result.to_crs({'init': 'epsg:26971'})\n",
    "output_map(result, pop_data, hospitals, resource_dropdown.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classified Accessibility Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "to be written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Luo, W., & Qi, Y. (2009). An enhanced two-step floating catchment area (E2SFCA) method for measuring spatial accessibility to primary care physicians. Health & place, 15(4), 1100-1107."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
